// TDF - Tuple Data Format
// Written by - Leo Englert
// Version - 1.0
// Description - provides parsing methods

// Example for TDF: (Files, (File, "file.txt"), (File, "file.pdf"), (File, "file.gsm"))
// Language definition:
//	expression ::= <tuple> | <value>
//	tuple ::= '(' <enum> ')'
//	enum  ::= <expression> ',' <expression> | <expression>
//	value ::= <string> | <int>Â | <float>

TDF_Parser(
	TDF_TokenList tokens,
	int count
) table;

TDF_createParser(TDF_TokenList tokens) TDF_Parser = TDF_Parser(tokens, 0)

TDF_destroyParser(TDF_Parser parser) {
	drop(parser)
}

TDF_hasNextToken(TDF_Parser parser) bool {
	count := parser.count
	tokens := parser.tokens
	size  := tokens.size
	return count < size
}

TDF_peekNextToken(TDF_Parser parser) TDF_Token {
	if !parser.TDF_hasNextToken() return *TDF_EMPTY_TOKEN()
	return parser.tokens.TDF_tokenAt(0)
}

TDF_nextToken(TDF_Parser parser) TDF_Token {
	if !parser.TDF_hasNextToken() return *TDF_EMPTY_TOKEN()
	return parser.tokens.TDF_removeElement(0)
}

parseTDF(txt text) txt {

	tokenizer := TDF_createTokenizer(text)
	tokens := tokenizer.TDF_tokenize()
	
	parser := TDF_createParser(tokens)
	tp := parser.TDF_parseExpression()

	// Clean Up
	parser.TDF_destroyParser()
	tokenizer.TDF_destroyTokenizer()

	return tp
}

TDF_parseExpression(TDF_Parser parser) txt {
	tp := tuple@std()

	

	return tp
}

// TokenType begin:

// Usage -> *TT_NONE()
TT_NONE()    		Ref = 0u
TT_LPARENT() 		Ref = 1u
TT_RPARENT() 		Ref = 2u
TT_COMMA()   		Ref = 3u
TT_NEWLINE()        Ref = 4u
TT_INTEGER()        Ref = 5u
TT_FLOAT()          Ref = 6u
TT_STRING()			Ref = 7u

TDF_SIMPLE_TOKENS := "(),\n"
TDF_SIMPLE_TOKENS_TT_LOOKUP is ubyte[ *TT_LPARENT(), *TT_RPARENT(), *TT_COMMA(), *TT_NEWLINE() ]
// :TokenType end

TDF_Token(
	txt value,
	int token_type,
	int start,
	int end
) table;

TDF_destroyToken(TDF_Token token) {
	drop(token)
}

TDF_EMPTY_TOKEN() TDF_Token = TDF_Token("", *TT_NONE(), 0, 0)


// We need this TokenList in order to store multiple
// tokens for later processing in the parser

// TokenList begin:
TDF_TokenListNode(
	TDF_Token token,
	TDF_TokenListNode next
) table;

TDF_TokenList(
	TDF_TokenListNode head,
	int size
) table;

TDF_createTokenList() TDF_TokenList = TDF_TokenList(null, 0)

TDF_destroyTokenList(TDF_TokenList list) {
	for list.size
		list.TDF_removeElement(0).TDF_destroyToken()

	drop(list)
}

TDF_pushBackToken(TDF_TokenList list, TDF_Token token) {
	list.head = TDF_TokenListNode(token, list.head)
	list.size += 1u
}

TDF_appendToken(TDF_TokenList list, TDF_Token token) {
	if isNull(list.head) {
		list.head = TDF_TokenListNode(token, null)
	} else {
		node := list.head
		for !fmt(node.next).cmp@std(null)
			node = node.next
		node.next = TDF_TokenListNode(token, null)
	}

	list.size += 1u
}

TDF_popFirstToken(TDF_TokenList list) TDF_Token {
	if list.size == 0u abort@std("Pop first Token from List: Out of Bounds!")
	
	token := list.head
	list.head = token.next

	list.size -= 1u

	return token
}

TDF_tokenAt(TDF_TokenList list, int index) TDF_Token {
	if list.size <= index abort@std("Access Token from List: Out of Bounds!")

	node := list.head

	for i = 0u; i < index; i += 1u {
		node = node.next
	}

	return node.token
}

TDF_removeElement(TDF_TokenList list, int index) TDF_Token {
	if list.size <= index abort@std("Remove Token from List: Out of Bounds!")

	before := list.head
	node := list.head

	for i = 0u; i < index; i += 1u {
		before = node
		node = node.next
	}

	if fmt(before).cmp@std(fmt(node))
		node.next = null
	else
		before.next = node.next

	list.size -= 1u
	return node.token
}

TDF_prettyPrintTokens(TDF_TokenList list) {
	token_type_reprs is txt[
		"[None]",
		"[(]",
		"[)]",
		"[,]",
		"[\\n]",
		"[int]",
		"[float]",
		"[string]"
	]
	
	for i = 0; i < list.size; i += 1 {
		token := list.TDF_tokenAt(i)
		reprs := token_type_reprs[token.token_type]
		value := token.value

		reprs.Print()
		if !reprs.cmp@std("[" + value + "]") && !reprs.cmp@std("[\\n]")
			fmt("(", value.StringRepresentation@std(), ")", " ").Print()
		else
			" ".Print()
		// fmt(reprs, "(", value, ")", " ").Print()
	}
	
	// Newline at the end
	output()
}

// :TokenList end

// Tokenizer begin:

TDF_Tokenizer(
	txt text,
	int count
) table;

TDF_createTokenizer(txt text) TDF_Tokenizer = TDF_Tokenizer(text, 0)

TDF_destroyTokenizer(TDF_Tokenizer tokenizer) {
	drop(tokenizer)
}

TDF_hasNextChar(TDF_Tokenizer tokenizer) bool = tokenizer.count < tokenizer.text.Length()

TDF_peekNextChar(TDF_Tokenizer tokenizer) ubyte {
	if !(tokenizer.TDF_hasNextChar()) return '\0'
	tokenizer.text.CharAt(tokenizer.count)
}

TDF_nextChar(TDF_Tokenizer tokenizer) ubyte {
	if !(tokenizer.TDF_hasNextChar()) return '\0'
	character := tokenizer.text.CharAt(tokenizer.count)
	tokenizer.count += 1
	return character
}

TDF_tokenize(TDF_Tokenizer tokenizer) TDF_TokenList {
	
	tokens := TDF_createTokenList()

	for tokenizer.TDF_hasNextChar() {
		character := tokenizer.TDF_peekNextChar()
		
		// check for characters
		if character.isInText@std(TDF_SIMPLE_TOKENS) {
			tokens.TDF_appendToken( tokenizer.TDF_nextSimpleToken() )
		} else if character.isInText@std(alpha@std + "!&=?+*^<>:#%.@_$-/\\") || character.isInText@std("\"'") {
			tokens.TDF_appendToken( tokenizer.TDF_nextString() )
		} else if character.isNumeric@std() {
			tokens.TDF_appendToken( tokenizer.TDF_nextNumber() )
		} else if character.isInText@std(whitespace@std) {
			tokenizer.TDF_nextChar()
		} else {
			logWarning@std(fmt("[TDF] Syntax Error: Unknown character '", Str@std(character), "'"))

			// But continue, because we don't want to
			// exit that fast and stop the process
			tokenizer.TDF_nextChar()
		}
	}
	
	return tokens
}

TDF_nextIdentifier(TDF_Tokenizer tokenizer) TDF_Token {
	identifier := ""
	token_type := *TT_STRING()
	start := tokenizer.count

	for tokenizer.TDF_hasNextChar() && (tokenizer.TDF_peekNextChar().isInText@std(nummeric@std + alpha@std + "!&=?+*^<>:#%.@_$-/\\")) {
		identifier = identifier.InsertChar(tokenizer.TDF_nextChar())
	}

	end := tokenizer.count

	if identifier.cmp@std("true") {
		identifier = "1"
		token_type = *TT_INTEGER()
	} else if identifier.cmp@std("false") {
		identifier = "0"
		token_type = *TT_INTEGER()
	}
	
	return TDF_Token(
		identifier,
		token_type,
		start, end
	)
}

TDF_nextString(TDF_Tokenizer tokenizer) TDF_Token {

	if !tokenizer.TDF_peekNextChar().isInText@std("'\"")
		return tokenizer.TDF_nextIdentifier()
	
	start := tokenizer.count
	string := ""
	stop_char := tokenizer.TDF_nextChar()
	
	for tokenizer.TDF_hasNextChar() && (tokenizer.TDF_peekNextChar() != stop_char) {
		character := tokenizer.TDF_nextChar()
		if character == '\\' && tokenizer.TDF_hasNextChar() {
			next_character := tokenizer.TDF_nextChar()

			if next_character == 'n' character = '\n'
			else if next_character == 't' character = '\t'
			else if next_character == 'a' character = '\a'
			else if next_character == 'r' character = '\r'
			else if next_character == 'b' character = '\b'
			else if next_character == 'x' character = '\x'
			else if next_character == '\\' character = '\\'
			else if next_character == stop_char character = stop_char
			else if next_character.isNumeric@std() {
				ascii_num := Str@std(next_character)
				for i = 0u; i < 3 && tokenizer.TDF_hasNextChar(); i += 1u {
					if tokenizer.TDF_peekNextChar().isNumeric@std() {
						ascii_num = ascii_num.InsertChar(tokenizer.TDF_nextChar())
					}
				}

				character = (ascii_num as ubyte)
			} else character = '?'
		}
		string = string.InsertChar(character)
	}

	tokenizer.TDF_nextChar()

	end := tokenizer.count

	return TDF_Token(
		string,
		*TT_STRING(),
		start, end
	)
}

TDF_nextNumber(TDF_Tokenizer tokenizer) TDF_Token {

	start := tokenizer.count
	number := Str@std(tokenizer.TDF_nextChar())
	token_type := *TT_INTEGER()

	for tokenizer.TDF_hasNextChar() && (tokenizer.TDF_peekNextChar().isInText@std(nummeric@std + ".")) {
		if tokenizer.TDF_peekNextChar() == '.' {
			if token_type == *TT_FLOAT()
				break
			else
				token_type = *TT_FLOAT()
		}
		number = number.InsertChar(tokenizer.TDF_nextChar())
	}

	end := tokenizer.count

	return TDF_Token(
		number,
		token_type,
		start, end
	)
}

TDF_nextSimpleToken(TDF_Tokenizer tokenizer) TDF_Token {
	start := tokenizer.count
	character := tokenizer.TDF_nextChar()
	end := tokenizer.count

	for i = 0u; i < TDF_SIMPLE_TOKENS.Length(); i += 1u
		if character == TDF_SIMPLE_TOKENS.CharAt(i)
			return TDF_Token(
				Str@std(character),
				TDF_SIMPLE_TOKENS_TT_LOOKUP[i],
				start, end
			)
	return *TDF_EMPTY_TOKEN()
}

// :Tokenizer end